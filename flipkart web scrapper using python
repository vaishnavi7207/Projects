# Project1
# Import all the required libraries
import pandas as pd
import requests
from bs4 import BeautifulSoup
# ask user for the input
search_term=input("Enter what you want to buy:") 
# make the search term url-friendly
search_term=search_term.replace(" ","+") 
# make empty lists of the values we want to extract
Product_name=[]
Prices=[]
Description=[]
Reviews=[]
# iterate through different pages of the website
for i in range(1,11):
  url="https://www.flipkart.com/search?q={}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=" 
# append the url with search term and page number
  url=url.format(search_term)+str(i) 
# make request object for sending https request and establishing a connection with the website
  r=requests.get(url)  
# make BeautifulSoup object to parse the html content
  soup=BeautifulSoup(r.text,"html.parser") 
# get number of all the elements in classes
  rect=soup.find_all("div",class_="_13oc-S")
# iterate through the classes and extract the required informations
  for r in rect:
    name=r.find("div",class_="_4rR01T")
    if(name==None):
      Product_name.append("None")
    else:
      Product_name.append(name.text)

    reviews=r.find("div",class_="_3LWZlK")
    if(reviews==None):
      Reviews.append("None")
    else:
      Reviews.append(reviews.text)

    desc=r.find("ul",class_="_1xgFaf")
    if(desc==None):
      Description.append("None")
    else:
      Description.append(desc.text)

    price=r.find("div",class_="_30jeq3 _1_WHN1")
    if(price==None):
      Prices.append("None")
    else:
      Prices.append(price.text.replace("â‚¹",""))
# total number of outputs
print(len(Product_name))
print(len(Reviews))
print(len(Prices))
print(len(Description))
# undoing search term for csv file names
search_term=search_term.replace("+"," ")
# making pandas dataframe and getting csv file
df=pd.DataFrame({"Product Name":Product_name,"Description":Description,"Prices(INR)":Prices,"Stars(Out of 5)":Reviews}) 
df.to_csv(search_term+".csv") 

#This code can be modified for extracting from different websites and may even fail to extract data due to changed policies of the respective websites.
